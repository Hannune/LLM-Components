# Open Interpreter Service Configuration

# Permissions
permissions:
  # Auto-run code without asking (use with caution!)
  auto_run: false
  
  # Safe mode: 'off', 'ask', or 'auto'
  # 'ask' - prompts before running code
  # 'auto' - safe operations only
  # 'off' - no restrictions (dangerous!)
  safe_mode: 'ask'

# Local LLM Configuration
llm:
  model: 'ollama/qwen2.5:7b'
  api_base: 'http://host.docker.internal:11434'
  temperature: 0.1
  
  # Alternative models:
  # model: 'ollama/llama3.1:8b'
  # model: 'ollama/codellama:13b'

# System message
system_message: |
  You are a helpful AI assistant with access to Python code execution.
  Be careful and explain what code you will run before executing.
  Always prioritize safety and user data protection.

# Resource limits (optional, Docker-level)
resources:
  max_memory: '2g'
  max_cpu: '2'
